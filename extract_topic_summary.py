from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import nltk
import re
from collections import defaultdict
from text_manager import nlp


class Summarizer:
    def __init__(self, model_dir="lcw99/t5-base-korean-text-summary"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
        self.max_input_length = 2048

    def summarize(self, text, max_length=128):
        inputs = self.tokenizer([text], max_length=self.max_input_length,
                                truncation=True, return_tensors="pt", padding=True)
        output = self.model.generate(
            **inputs, num_beams=16, do_sample=False, min_length=1, max_length=max_length)
        decoded = self.tokenizer.batch_decode(
            output, skip_special_tokens=True)[0]
        return nltk.sent_tokenize(decoded.strip())[0]


def remove_parentheses_content(text: str) -> str:
    # ê´„í˜¸ìŒ: (), [], <>, ã€ˆã€‰, ã€Šã€‹
    pattern = r'[\(\[\<ã€ˆã€Š][^)\]\>ã€‰ã€‹]*[\)\]\>ã€‰ã€‹]'
    cleaned = re.sub(pattern, '', text)
    return re.sub(r'\s{2,}', ' ', cleaned).strip()


import re

def remove_parentheses_content(text: str) -> str:
    pattern = r'[\(\[\<ã€ˆã€Š][^)\]\>ã€‰ã€‹]*[\)\]\>ã€‰ã€‹]'
    cleaned = re.sub(pattern, '', text)
    return re.sub(r'\s{2,}', ' ', cleaned).strip()


def restore_names_from_original(original: str, summary: str) -> str:
    POSITION_SUFFIXES = ["ì˜ì›", "ì¥", "ì „", "ë‹¹", "ëŒ€í‘œ", "ìˆ˜ì„"]
    MAX_NAME_BLOCK = 4  # ìµœëŒ€ 4ë‹¨ì–´ê¹Œì§€ ì´ë¦„ ë¸”ë¡ìœ¼ë¡œ ê°„ì£¼

    def split_words(text):
        return re.findall(r'\b[\wê°€-í£]+\b', text)

    def get_position_suffix(word: str) -> str | None:
        for suffix in POSITION_SUFFIXES:
            if suffix in word:
                return suffix
        return None
    
    def ends_with_particle(text):
        return text.endswith(("ì€", "ëŠ”", "ì´", "ê°€", "ì™€", "ê³¼", "ë„"))

    original = remove_parentheses_content(original)
    original_words = split_words(original)
    summary_words = split_words(summary)

    # ì›ë¬¸ì—ì„œ 2~4ë‹¨ì–´ì”© ë¸”ë¡ ì¶”ì¶œ
    original_blocks = []
    for i in range(len(original_words)):
        for size in range(2, MAX_NAME_BLOCK + 1):
            if i + size <= len(original_words):
                block = original_words[i:i + size]
                original_blocks.append(block)

    # ìš”ì•½ë¬¸ 2ë‹¨ì–´ìŒ
    summary_pairs = [(summary_words[i], summary_words[i + 1])
                     for i in range(len(summary_words) - 1)]

    replacement_map = {}

    for block in original_blocks:
        if len(block) < 2:
            continue
        full_name = ' '.join(block)
        o1 = block[0]
        o2 = block[-1]  # ì§ì±… ì¶”ì •

        for s1, s2 in summary_pairs:
            suffix_o = get_position_suffix(o2)
            suffix_s = get_position_suffix(s2)
            if (
                o1[0] == s1 and
                (o2 == s2 or (suffix_o and suffix_o == suffix_s)) and
                len(o1) >= 2 and
                len(o1) <= 3
            ):
                short_form = f"{s1} {s2}"
                if (
                    short_form not in replacement_map or
                    len(full_name) < len(replacement_map[short_form])
                ):
                    replacement_map[short_form] = full_name

    print(replacement_map)

    # ì‹¤ì œ ì¹˜í™˜
    for short, full in replacement_map.items():
        if short in full:
            continue
        if (ends_with_particle(short) and ends_with_particle(full)) or \
            (not ends_with_particle(short) and not ends_with_particle(full)):
            summary = summary.replace(short, full)

    return summary


class RedundancyRemover:
    def __init__(self, min_common_len=3):
        self.min_common_len = min_common_len
        self._init_nlp()

    def _init_nlp(self):
        # stanza.download('ko')
        # self.nlp = stanza.Pipeline(
        #     lang='ko', processors='tokenize,pos,lemma', verbose=False)
        self.nlp = nlp

    def tokenize(self, text: str):
        doc = self.nlp(text)
        return [word.text for sent in doc.sentences for word in sent.words]

    def lemmatize(self, text: str):
        doc = self.nlp(text)
        return [word.lemma.split('+')[0] for sent in doc.sentences for word in sent.words]

    def trim_redundant_block(self, text: str) -> str:
        tokens = self.tokenize(text)
        lemmas = self.lemmatize(text)

        # lemma -> ëª¨ë“  ë“±ì¥ ì¸ë±ìŠ¤ ê¸°ë¡
        lemma_map = defaultdict(list)
        for idx, lemma in enumerate(lemmas):
            lemma_map[lemma].append(idx)

        # ì—°ì†ëœ ë°˜ë³µ êµ¬ê°„ í›„ë³´ ì°¾ê¸°
        max_start, max_end = -1, -1
        max_len = 0

        for lemma, indices in lemma_map.items():
            if len(indices) < 2:
                continue
            # ëª¨ë“  ê°€ëŠ¥í•œ (i, j) ìŒ ë¹„êµ (i < j)
            for i in range(len(indices)):
                for j in range(i + 1, len(indices)):
                    start1, start2 = indices[i], indices[j]
                    length = 0
                    while (start1 + length < start2 and
                           start2 + length < len(lemmas) and
                           lemmas[start1 + length] == lemmas[start2 + length]):
                        length += 1
                    if length >= self.min_common_len and length > max_len:
                        max_len = length
                        max_start = start1
                        max_end = start1 + length

        # ì œê±°í•  ì¤‘ë³µ êµ¬ê°„ì´ ìˆë‹¤ë©´ ì œê±°
        if max_len >= self.min_common_len:
            new_tokens = tokens[:max_start] + tokens[max_end:]
            return ' '.join(new_tokens).replace(" .", ".")

        return text


# class RedundancyRemover:
#     POSITION_SUFFIXES = ["ì˜ì›", "ì¥", "ë‹¹", "ëŒ€í‘œ", "ìˆ˜ì„"]
#     def __init__(self, min_common_len=3):
#         self.min_common_len = min_common_len
#         self._init_nlp()

#     def _init_nlp(self):
#         # self.nlp = stanza.Pipeline(...)
#         self.nlp = nlp  # ì™¸ë¶€ì—ì„œ ì£¼ì…í•œ stanza Pipeline

#     def tokenize(self, text: str):
#         doc = self.nlp(text)
#         return [word.text for sent in doc.sentences for word in sent.words]

#     def lemmatize(self, text: str):
#         doc = self.nlp(text)
#         # print(doc)
#         return [word.lemma.split('+')[0] for sent in doc.sentences for word in sent.words]

#     def trim_redundant_block(self, text: str) -> str:
#         while True:
#             tokens = self.tokenize(text)
#             lemmas = self.lemmatize(text)
            
#             print(lemmas)

#             # lemma â†’ ë“±ì¥ ì¸ë±ìŠ¤ ê¸°ë¡
#             lemma_map = defaultdict(list)
#             for idx, lemma in enumerate(lemmas):
#                 lemma_map[lemma].append(idx)

#             # ê°€ì¥ ê¸´ ë°˜ë³µ êµ¬ê°„ íƒìƒ‰
#             max_start, max_end, max_len = -1, -1, 0

#             for lemma, indices in lemma_map.items():
#                 if len(indices) < 2:
#                     continue
#                 for i in range(len(indices)):
#                     for j in range(i + 1, len(indices)):
#                         start1, start2 = indices[i], indices[j]
#                         length = 0
#                         while (start1 + length < start2 and
#                                start2 + length < len(lemmas) and
#                                lemmas[start1 + length] == lemmas[start2 + length]):
#                             length += 1
#                         if length >= self.min_common_len and length > max_len:
#                             max_len = length
#                             max_start = start1
#                             max_end = start1 + length

#             # ì œê±°í•  ì¤‘ë³µ êµ¬ê°„ì´ ì—†ë‹¤ë©´ ì¢…ë£Œ
#             if max_len < self.min_common_len:
#                 break

#             # ì¤‘ë³µ êµ¬ê°„ ì œê±°
#             tokens = tokens[:max_start] + tokens[max_end:]

#             text = ' '.join(tokens).replace(" .", ".")

#         return text

class TopicExtractor:
    def __init__(self):
        self.summarizer = Summarizer()
        self.remover = RedundancyRemover()

    def extract_topic(self, title=None, body=None, purpose=None, sentence=None, name=None):
        summary = self.summarizer.summarize(body.replace("\n", " "))
        print(f"\nìš”ì•½ ê²°ê³¼:\t{summary}")

        # ë³¸ë¬¸ì´ ì—†ëŠ” ê²½ìš° ë¹ˆì¹¸ ë°˜í™˜
        if body == "" or "nan" in summary:
            return ""

        removed = self.remover.trim_redundant_block(summary)
        print(f"ì¤‘ë³µ ì œê±°:\t{removed}")

        replaced = restore_names_from_original(body, removed)
        print(f"ì´ë¦„ ë³µì›:\t{replaced}")

        return replaced


# ğŸ” ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":
    title = "ê¹€ ì˜ì›, ì¥ì• ì¸ì˜ˆìˆ ë‹¨ ì„¤ë¦½ ì§ˆì˜"
    body1 = """
ë¯¼ì£¼ë‹¹ ì˜ì›ë“¤ì€ ì§‘íšŒ ì°¸ì„ì— ì´ì–´ ì‚¬íšŒê´€ê³„ë§ì„œë¹„ìŠ¤(SNS)ë¥¼ í†µí•´ì„œë„ ì •ë¶€Â·ì—¬ë‹¹ì„ í–¥í•œ ê·œíƒ„ ë©”ì‹œì§€ë¥¼ ì•ë‹¤í‰ˆ ìŸì•„ëƒˆë‹¤. ì´ì—°í¬ ì˜ì›ì€ "ì´ì„ ì—ì„œ êµ­ë¯¼ì´ ì‹¬íŒí–ˆëŠ”ë° ëŒ€í†µë ¹ì´ ë“£ì§€ ì•ŠëŠ”ë‹¤ë©´ êµ­ë¯¼ë“¤ì´ ë‚˜ì„œì•¼ í•œë‹¤"ë©° "ìœ¤ì„ì—´ ì •ê¶Œì´ êµ­ì • ê¸°ì¡°ë¥¼ ì „í™˜í•˜ê³  ì¸ì  ì‡„ì‹ ì„ ì´ë£° ë•Œê¹Œì§€ êµ­ë¯¼ë“¤ì´ ë‚˜ì„œì„œ ìœ¤ ëŒ€í†µë ¹ì„ êµ´ë³µì‹œì¼œì•¼ í•œë‹¤. ê·¸ ê¸¸ì— ë¯¼ì£¼ë‹¹ì´ ì•ì¥ì„¤ ê²ƒ"ì´ë¼ê³  í–ˆë‹¤. ìœ¤ê±´ì˜ ì˜ì›ì€ "ì •ë¶€ì™€ ì—¬ë‹¹ì€ í•œ ëª¸ìœ¼ë¡œ í•´ë³‘ëŒ€ì› íŠ¹ê²€ë²•ì„ ê±°ë¶€í–ˆë‹¤. ì§„ì‹¤ì„ ìˆ¨ê¸°ê³  ìê¸° ìì‹ ë§Œ ì§€í‚¤ê¸° ìœ„í•œ í•©ë™ ê¶Œí•œë‚¨ìš© ì‘ì „"ì´ë¼ë©° "ëê¹Œì§€ ìˆ¨ê¸¸ ìˆ˜ ìˆëŠ” ì§„ì‹¤ì€ ì—†ë‹¤"ê³  ê°•ì¡°í–ˆë‹¤. ì—¼íƒœì˜ ì˜ì›ì€ "êµ­ë°©ì˜ ì˜ë¬´ë¥¼ ë‹¤í•˜ë‹¤ ìˆœì§í•œ í•œ ì Šì€ êµ°ì¸ê³¼ ê·¸ ê°€ì¡±ë“¤ì˜ í•œì„ í’€ ìˆ˜ ìˆë„ë¡ í•´ë‹¬ë¼"ë©° "ì†ë°”ë‹¥ìœ¼ë¡œ í•˜ëŠ˜ì„ ê°€ë¦¬ë ¤ëŠ” ëŒ€í†µë ¹ê³¼ ì—¬ë‹¹ì„ êµ­ë¯¼ì˜ ë§¤ì„œìš´ íšŒì´ˆë¦¬ë¡œ ì‘ì§•í•´ë‹¬ë¼"ê³  í˜¸ì†Œí–ˆë‹¤. ê¹€ë™ì•„ ì˜ì›ì€ "(ì •ë¶€Â·ì—¬ë‹¹ì´) ê¶Œë ¥ì„ ì‚¬ì ìœ¼ë¡œ ì•…ìš©í•˜ëŠ” ëª¨ìŠµì„ ë” ì´ìƒ ìš°ë¦¬ëŠ” ìš©ë‚©í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì‹ ì†í•˜ê³  ê°•ë ¥í•˜ê²Œ êµ­ë¯¼ì´ ìœ„ì„í•œ ê¶Œí•œì„ í–‰ì‚¬í•´ë‚˜ê°ˆ ê²ƒ"ì´ë¼ê³  í–ˆë‹¤.

"""

    extractor = TopicExtractor()
    topic = extractor.extract_topic(title=title, body=body1)
